{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16fb28ff-a27f-41c1-a203-122e7486a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7f465-12e7-4c08-9641-e9743d1555b3",
   "metadata": {},
   "source": [
    "**Step 1: Create the embeding representation of our sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c525216-3377-44ce-a8f1-63e6799e42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {\n",
    "    'she':    np.array([0.9, 0.2, 0.1, 0.1]),  # High first dim for subject\n",
    "    'likes':  np.array([0.1, 0.9, 0.2, 0.1]),  # High second dim for verb\n",
    "    'coffee': np.array([0.1, 0.2, 0.9, 0.1])   # High third dim for object\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f123598-eaae-493d-b252-0a98d4d27b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Word Embeddings\n",
      "she: [0.9 0.2 0.1 0.1]\n",
      "likes: [0.1 0.9 0.2 0.1]\n",
      "coffee: [0.1 0.2 0.9 0.1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1 - Word Embeddings\")\n",
    "for word, embed in word_embeddings.items():\n",
    "    print(f\"{word}: {embed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6182dba-6c2b-45ea-8101-650f53787e88",
   "metadata": {},
   "source": [
    "**Step 2: Create input matrix X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a6d5b6b-9dde-439c-ba2c-be4fef9afa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9, 0.2, 0.1, 0.1],\n",
       "       [0.1, 0.9, 0.2, 0.1],\n",
       "       [0.1, 0.2, 0.9, 0.1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.vstack([word_embeddings['she'], \n",
    "               word_embeddings['likes'], \n",
    "               word_embeddings['coffee']])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b36f34a7-8495-43e1-b481-c9558ed6a07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Input Matrix X\n",
      "[[0.9 0.2 0.1 0.1]\n",
      " [0.1 0.9 0.2 0.1]\n",
      " [0.1 0.2 0.9 0.1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2: Input Matrix X\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff2ea8-b263-4cc3-a6af-5e583e0659b1",
   "metadata": {},
   "source": [
    "**Step 3: Initialization of W_q, W_k, W_v**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48d4ff8e-5416-417f-ab4a-47fdbb626b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.9, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.9, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.9, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.9]]),\n",
       " array([[0.9, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.9, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.9, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.9]]),\n",
       " array([[0.8, 0.2, 0.1, 0.1],\n",
       "        [0.2, 0.8, 0.2, 0.1],\n",
       "        [0.1, 0.2, 0.8, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.9]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q = np.array([[0.9, 0.1, 0.1, 0.1],\n",
    "                [0.1, 0.9, 0.1, 0.1],\n",
    "                [0.1, 0.1, 0.9, 0.1],\n",
    "                [0.1, 0.1, 0.1, 0.9]])\n",
    "\n",
    "W_k = np.array([[0.9, 0.1, 0.1, 0.1],\n",
    "                [0.1, 0.9, 0.1, 0.1],\n",
    "                [0.1, 0.1, 0.9, 0.1],\n",
    "                [0.1, 0.1, 0.1, 0.9]])\n",
    "\n",
    "W_v = np.array([[0.8, 0.2, 0.1, 0.1],\n",
    "                [0.2, 0.8, 0.2, 0.1],\n",
    "                [0.1, 0.2, 0.8, 0.1],\n",
    "                [0.1, 0.1, 0.1, 0.9]])\n",
    "W_q, W_k, W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3fc341b-7962-4202-9f68-7aa7005ed433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.85, 0.29, 0.21, 0.21],\n",
       "        [0.21, 0.85, 0.29, 0.21],\n",
       "        [0.21, 0.29, 0.85, 0.21]]),\n",
       " array([[0.85, 0.29, 0.21, 0.21],\n",
       "        [0.21, 0.85, 0.29, 0.21],\n",
       "        [0.21, 0.29, 0.85, 0.21]]),\n",
       " array([[0.78, 0.37, 0.22, 0.21],\n",
       "        [0.29, 0.79, 0.36, 0.21],\n",
       "        [0.22, 0.37, 0.78, 0.21]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.dot(X, W_q)\n",
    "K = np.dot(X, W_k)\n",
    "V = np.dot(X, W_v)\n",
    "Q, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22273e-717a-4616-a0f5-154b2934266a",
   "metadata": {},
   "source": [
    "**Step 4: Q, K, V matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37fef48f-c307-4443-8f26-f1f8bbcd1fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Q, K, V matrices\n",
      "Q (Query matrix):\n",
      "[[0.85 0.29 0.21 0.21]\n",
      " [0.21 0.85 0.29 0.21]\n",
      " [0.21 0.29 0.85 0.21]]\n",
      "\n",
      "K (Key matrix):\n",
      "[[0.85 0.29 0.21 0.21]\n",
      " [0.21 0.85 0.29 0.21]\n",
      " [0.21 0.29 0.85 0.21]]\n",
      "\n",
      "V (Value matrix):\n",
      "[[0.78 0.37 0.22 0.21]\n",
      " [0.29 0.79 0.36 0.21]\n",
      " [0.22 0.37 0.78 0.21]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 4: Q, K, V matrices\")\n",
    "print(\"Q (Query matrix):\")\n",
    "print(np.round(Q, 3))\n",
    "print(\"\\nK (Key matrix):\")\n",
    "print(np.round(K, 3))\n",
    "print(\"\\nV (Value matrix):\")\n",
    "print(np.round(V, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0a41c-681c-4f07-afef-404c84ced442",
   "metadata": {},
   "source": [
    "**Step 5: Compute attention scores (Q × K^T)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7992364-977f-4fa2-8a30-0dbd0fa2e9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Raw attention scores (Q × K^T)\n",
      "[[0.895 0.53  0.485]\n",
      " [0.53  0.895 0.581]\n",
      " [0.485 0.581 0.895]]\n"
     ]
    }
   ],
   "source": [
    "scores = np.dot(Q, K.T)\n",
    "print(\"\\nStep 5: Raw attention scores (Q × K^T)\")\n",
    "print(np.round(scores, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf6ef8-f813-4e23-b8a7-19ac610128b9",
   "metadata": {},
   "source": [
    "**Step 6: Scale the scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "784b9a04-d687-4e29-9bc9-d5b39a614f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Scaled attention scores (divided by sqrt(d_k))\n",
      "[[0.447 0.265 0.243]\n",
      " [0.265 0.447 0.291]\n",
      " [0.243 0.291 0.447]]\n"
     ]
    }
   ],
   "source": [
    "d_k = K.shape[1]  # dimension of key vectors\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "print(\"\\nStep 6: Scaled attention scores (divided by sqrt(d_k))\")\n",
    "print(np.round(scaled_scores, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f006b-815b-4811-a493-b959e8ed4fc5",
   "metadata": {},
   "source": [
    "**Step 7: Apply softmax to get attention weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38de6162-2317-40c9-b28a-35ec75f93de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7: Attention weights after softmax\n",
      "[[0.378 0.315 0.308]\n",
      " [0.31  0.372 0.318]\n",
      " [0.305 0.32  0.375]]\n"
     ]
    }
   ],
   "source": [
    "exp_scores = np.exp(scaled_scores)\n",
    "attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
    "print(\"\\nStep 7: Attention weights after softmax\")\n",
    "print(np.round(attention_weights, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4373610-4f0b-48b8-bf96-f26773748ad7",
   "metadata": {},
   "source": [
    "**Step 8: Compute final output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf77b652-e64a-44fe-bfab-ff1252709015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 8: Final output\n",
      "[[0.454 0.502 0.436 0.21 ]\n",
      " [0.42  0.526 0.45  0.21 ]\n",
      " [0.413 0.504 0.475 0.21 ]]\n"
     ]
    }
   ],
   "source": [
    "output = np.dot(attention_weights, V)\n",
    "print(\"\\nStep 8: Final output\")\n",
    "print(np.round(output, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba062cb-b6b4-4500-9a7d-69886fc14d3b",
   "metadata": {},
   "source": [
    "**Visualization of attention weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff749ba9-fe3c-44ef-8a2d-fd52f90254f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention Weight Visualization:\n",
      "\n",
      "she attending to:\n",
      "     she: ███████ (0.378)\n",
      "   likes: ██████ (0.315)\n",
      "  coffee: ██████ (0.308)\n",
      "\n",
      "likes attending to:\n",
      "     she: ██████ (0.310)\n",
      "   likes: ███████ (0.372)\n",
      "  coffee: ██████ (0.318)\n",
      "\n",
      "coffee attending to:\n",
      "     she: ██████ (0.305)\n",
      "   likes: ██████ (0.320)\n",
      "  coffee: ███████ (0.375)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAttention Weight Visualization:\")\n",
    "words = ['she', 'likes', 'coffee']\n",
    "for i, word1 in enumerate(words):\n",
    "    print(f\"\\n{word1} attending to:\")\n",
    "    for j, word2 in enumerate(words):\n",
    "        weight = attention_weights[i][j]\n",
    "        bars = '█' * int(weight * 20)\n",
    "        print(f\"{word2:>8}: {bars} ({weight:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20f63b27-1142-48ae-924b-afa1c0fa44f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: colorama\n",
      "Successfully installed colorama-0.4.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "Attention Weight Visualization:\n",
      "\n",
      "\u001b[1mshe attending to:\u001b[0m\n",
      "     she: \u001b[33m███████\u001b[0m (0.378)\n",
      "   likes: \u001b[33m██████\u001b[0m (0.315)\n",
      "  coffee: \u001b[33m██████\u001b[0m (0.308)\n",
      "\n",
      "------------------------------\n",
      "\u001b[1mlikes attending to:\u001b[0m\n",
      "     she: \u001b[33m██████\u001b[0m (0.310)\n",
      "   likes: \u001b[33m███████\u001b[0m (0.372)\n",
      "  coffee: \u001b[33m██████\u001b[0m (0.318)\n",
      "\n",
      "------------------------------\n",
      "\u001b[1mcoffee attending to:\u001b[0m\n",
      "     she: \u001b[33m██████\u001b[0m (0.305)\n",
      "   likes: \u001b[33m██████\u001b[0m (0.320)\n",
      "  coffee: \u001b[33m███████\u001b[0m (0.375)\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "!pip install colorama\n",
    "from colorama import Fore, Style\n",
    "\n",
    "words = ['she', 'likes', 'coffee']\n",
    "\n",
    "# Function to get color based on weight for emphasis\n",
    "def get_color(weight):\n",
    "    if weight > 0.5:\n",
    "        return Fore.GREEN\n",
    "    elif weight > 0.3:\n",
    "        return Fore.YELLOW\n",
    "    else:\n",
    "        return Fore.RED\n",
    "\n",
    "print(\"\\nAttention Weight Visualization:\\n\")\n",
    "for i, word1 in enumerate(words):\n",
    "    print(f\"{Style.BRIGHT}{word1} attending to:{Style.RESET_ALL}\")\n",
    "    for j, word2 in enumerate(words):\n",
    "        weight = attention_weights[i][j]\n",
    "        bars = '█' * int(weight * 20)\n",
    "        color = get_color(weight)\n",
    "        print(f\"{word2:>8}: {color}{bars}{Style.RESET_ALL} ({weight:.3f})\")\n",
    "    print(\"\\n\" + \"-\" * 30)  # Separator between words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4967c1a0-827e-4df4-8a3f-5c4920ebdbcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Initialize and run multi-head attention\u001b[39;00m\n\u001b[1;32m     79\u001b[0m mha \u001b[38;5;241m=\u001b[39m MultiHeadAttention(d_model\u001b[38;5;241m=\u001b[39md_model, num_heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[0;32m---> 80\u001b[0m output, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmha\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Print attention patterns for each head\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAttention patterns per head:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 57\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m K_head \u001b[38;5;241m=\u001b[39m K[:, :, h, :]\n\u001b[1;32m     55\u001b[0m V_head \u001b[38;5;241m=\u001b[39m V[:, :, h, :]\n\u001b[0;32m---> 57\u001b[0m output, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_head\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m head_outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     59\u001b[0m head_weights\u001b[38;5;241m.\u001b[39mappend(weights)\n",
      "Cell \u001b[0;32mIn[35], line 23\u001b[0m, in \u001b[0;36mMultiHeadAttention.attention\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Scaled dot-product attention\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate attention scores\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(Q, \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2229889-68ca-4f66-a445-37a8c246a459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
